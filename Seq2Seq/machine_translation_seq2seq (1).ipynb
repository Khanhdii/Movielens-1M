{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b88471",
   "metadata": {},
   "source": [
    "# Machine Translation with Seq2Seq Model\n",
    "\n",
    "In this notebook, we will explore the fascinating world of **Machine Translation** using a **Seq2Seq model**. Our goal is to build a model that can translate text from one language to another.\n",
    "\n",
    "Key Highlights:\n",
    "\n",
    "1. **Seq2Seq Model**: We will be using a Sequence-to-Sequence model, a type of model that converts an input sequence into an output sequence. It's widely used in tasks such as machine translation, speech recognition, and more.\n",
    "\n",
    "2. **Beam Search**: To improve the quality of our translations, we will implement Beam Search, a heuristic search algorithm that explores the most promising nodes.\n",
    "\n",
    "3. **BLEU Score**: To evaluate the performance of our model, we will use the Bilingual Evaluation Understudy (BLEU) score. It's a popular metric for machine translation that compares the translated text with the reference text.\n",
    "\n",
    "Stay tuned as we dive into the code and unravel the intricacies of machine translation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3cd99",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "The data we will be using for this project is the **Bilingual Sentence Pairs** dataset, which can be found at the following link:\n",
    "\n",
    "[https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs](https://www.kaggle.com/datasets/alincijov/bilingual-sentence-pairs)\n",
    "\n",
    "This dataset contains pairs of sentences in different languages, making it an excellent resource for our machine translation task."
   ]
  },
  {
   "cell_type": "code",
   "id": "9242b7f5",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as pl\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab, GloVe\n",
    "from gensim.models import KeyedVectors\n",
    "from typing import Iterable, List, Callable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.text import BLEUScore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca2741c6",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23eb4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The data file contains multiple lines of text. Each line contains a pair of sentences, and an attribution information.\n",
    "    The three parts are separated by tab characters. This function reads the data file and returns a data frame.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): the name of the data file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: a data frame containing the data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read each line, split it by tab characters, and store the result in a list\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = [line.strip().split('\\t') for line in f.readlines()]\n",
    "        \n",
    "    # Some lines are empty, so we need to remove them\n",
    "    lines = [line for line in lines if len(line) == 3]\n",
    "    \n",
    "    # Convert the list to a data frame\n",
    "    df = pd.DataFrame(lines, columns=['english', 'french', 'attribution'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb18f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = read_text('Data/fra.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29be6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english   french                                        attribution\n",
       "0     Go.     Va !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1     Go.  Marche.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2     Go.  Bouge !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3     Hi.  Salut !  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "4     Hi.   Salut.  CC-BY 2.0 (France) Attribution: tatoeba.org #5..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a110c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have total 185583 sentence pairs in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f'We have total {len(df)} sentence pairs in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6faa9a9",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In this step, we will preprocess our data to make it suitable for our Seq2Seq model. We will use the spaCy library, which is a powerful tool for natural language processing. Specifically, we will use two models from Spacy: one for English and one for French. \n",
    "\n",
    "The preprocessing steps include:\n",
    "1. Removing punctuation: Punctuation can introduce unnecessary complexity into our model, so we will remove it.\n",
    "2. Converting to lower case: This ensures that our model does not treat the same word in different cases as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34299f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the spaCy model for English and French\n",
    "if not spacy.util.is_package('en_core_web_md'):\n",
    "    spacy.cli.download('en_core_web_md')\n",
    "    \n",
    "if not spacy.util.is_package('fr_core_news_md'):\n",
    "    spacy.cli.download('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96144571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy models\n",
    "nlp_en = spacy.load('en_core_web_md')\n",
    "nlp_fr = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba07905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179927</th>\n",
       "      <td>I can't concentrate if you keep tapping me on ...</td>\n",
       "      <td>Je ne peux pas me concentrer si tu continues d...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50731</th>\n",
       "      <td>Where are your things?</td>\n",
       "      <td>Où sont tes affaires ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113343</th>\n",
       "      <td>Tom has too many strange ideas.</td>\n",
       "      <td>Tom a trop d'idées étranges.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46173</th>\n",
       "      <td>How is your new class?</td>\n",
       "      <td>Comment est ta nouvelle classe ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150522</th>\n",
       "      <td>Such a thing has never happened before.</td>\n",
       "      <td>C'est du jamais-vu.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "179927  I can't concentrate if you keep tapping me on ...   \n",
       "50731                              Where are your things?   \n",
       "113343                    Tom has too many strange ideas.   \n",
       "46173                              How is your new class?   \n",
       "150522            Such a thing has never happened before.   \n",
       "\n",
       "                                                   french  \\\n",
       "179927  Je ne peux pas me concentrer si tu continues d...   \n",
       "50731                              Où sont tes affaires ?   \n",
       "113343                       Tom a trop d'idées étranges.   \n",
       "46173                    Comment est ta nouvelle classe ?   \n",
       "150522                                C'est du jamais-vu.   \n",
       "\n",
       "                                              attribution  \n",
       "179927  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "50731   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "113343  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "46173   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "150522  CC-BY 2.0 (France) Attribution: tatoeba.org #8...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print random 5 rows\n",
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27145587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185583/185583 [00:02<00:00, 63153.13it/s]\n",
      "100%|██████████| 185583/185583 [00:05<00:00, 32466.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Register the tqdm function with pandas to show a progress bar when applying the function to a dataframe\n",
    "tqdm.pandas()\n",
    "\n",
    "# Clean the English text\n",
    "df['english'] = df['english'].progress_apply(\n",
    "    lambda x: ' '.join([token.text.lower() for token in nlp_en.tokenizer(x) if token.is_alpha])\n",
    ")\n",
    "\n",
    "# Clean the French text\n",
    "df['french'] = df['french'].progress_apply(\n",
    "    lambda x: ' '.join([token.text.lower() for token in nlp_fr.tokenizer(x) if token.is_alpha])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76383bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179927</th>\n",
       "      <td>i ca concentrate if you keep tapping me on the...</td>\n",
       "      <td>je ne peux pas me concentrer si tu continues d...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50731</th>\n",
       "      <td>where are your things</td>\n",
       "      <td>où sont tes affaires</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113343</th>\n",
       "      <td>tom has too many strange ideas</td>\n",
       "      <td>tom a trop idées étranges</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46173</th>\n",
       "      <td>how is your new class</td>\n",
       "      <td>comment est ta nouvelle classe</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150522</th>\n",
       "      <td>such a thing has never happened before</td>\n",
       "      <td>est du jamais vu</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "179927  i ca concentrate if you keep tapping me on the...   \n",
       "50731                               where are your things   \n",
       "113343                     tom has too many strange ideas   \n",
       "46173                               how is your new class   \n",
       "150522             such a thing has never happened before   \n",
       "\n",
       "                                                   french  \\\n",
       "179927  je ne peux pas me concentrer si tu continues d...   \n",
       "50731                                où sont tes affaires   \n",
       "113343                          tom a trop idées étranges   \n",
       "46173                      comment est ta nouvelle classe   \n",
       "150522                                   est du jamais vu   \n",
       "\n",
       "                                              attribution  \n",
       "179927  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "50731   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "113343  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "46173   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "150522  CC-BY 2.0 (France) Attribution: tatoeba.org #8...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print random 5 rows\n",
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52cfa51",
   "metadata": {},
   "source": [
    "# TorchText\n",
    "\n",
    "[TorchText](https://pytorch.org/text/stable/index.html) is a PyTorch package that makes text processing easier and more convenient. It provides essential tools for preprocessing text data, including tokenization, building vocabulary, and batching data for input into a model.\n",
    "\n",
    "In this notebook, we will use TorchText for the following tasks:\n",
    "\n",
    "1. **Tokenization**: We will use the `get_tokenizer` function to create a tokenizer that splits our sentences into tokens (words).\n",
    "\n",
    "2. **Building Vocabulary**: We will use the `build_vocab_from_iterator` function to create a vocabulary from our dataset. This vocabulary will map each token to a unique integer, which our model can work with.\n",
    "\n",
    "3. **Text to Integer Sequence Conversion**: We will create a custom function `text_transform` that uses our vocabulary to convert our sentences into sequences of integers.\n",
    "\n",
    "4. **Batching**: When training our model, we will use the `BucketIterator` to create batches of our data. This will automatically handle padding of sequences to the same length within each batch.\n",
    "\n",
    "By using TorchText, we can greatly simplify the preprocessing of our text data and ensure that it is done in a way that is optimal for our PyTorch model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645a5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb102c3",
   "metadata": {},
   "source": [
    "The `yield_tokens` function is a generator function that tokenizes text data from an iterable (like a list or a DataFrame column) and yields the tokens one by one.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. The function takes two arguments: `data_iter`, which is an iterable of text data, and `tokenizer`, which is a callable (like a function) that takes a string and returns a list of tokens.\n",
    "\n",
    "2. The function iterates over `data_iter`.\n",
    "\n",
    "3. It applies the `tokenizer` to `text`, which splits the text into tokens.\n",
    "\n",
    "4. It then yields these tokens one by one. Because it's a generator function, it doesn't return all tokens at once but yields them one by one. This is memory-efficient when dealing with large amounts of text data.\n",
    "\n",
    "The reason for using this function is to create a stream of tokens from the text data. These tokens are used to build a vocabulary for text processing. The vocabulary maps each unique token to a unique integer, which can be used as input to a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61f0a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, tokenizer: Callable[[str], List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Yield the tokens from the data iterator.\n",
    "    \n",
    "    Args:\n",
    "        data_iter (Iterable): the data iterator\n",
    "        tokenizer (Callable[[str], List[str]]): the tokenizer\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: the tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c09f7",
   "metadata": {},
   "source": [
    "The `build_vocab_from_iterator` function in TorchText is used to build a vocabulary from an iterator that yields list or iterator of tokens. \n",
    "\n",
    "Here's a step-by-step explanation with a visualization example:\n",
    "\n",
    "1. The function takes an iterator of tokenized text data. This iterator could be a list of sentences, where each sentence is a list of tokens.\n",
    "\n",
    "2. The function iterates over this iterator, and for each list of tokens, it adds each token to the vocabulary.\n",
    "\n",
    "3. The vocabulary is essentially a dictionary where each unique token is a key and the corresponding value is a unique integer. The integer values are assigned in the order the tokens are encountered.\n",
    "\n",
    "4. The function returns this vocabulary.\n",
    "\n",
    "Here's a visualization example:\n",
    "\n",
    "Suppose we have the following tokenized text data:\n",
    "\n",
    "```\n",
    "[\n",
    "    ['I', 'love', 'coding'],\n",
    "    ['coding', 'is', 'fun'],\n",
    "    ['I', 'love', 'AI']\n",
    "]\n",
    "```\n",
    "\n",
    "The `build_vocab_from_iterator` function will build the following vocabulary from this data:\n",
    "\n",
    "```\n",
    "{\n",
    "    'I': 0,\n",
    "    'love': 1,\n",
    "    'coding': 2,\n",
    "    'is': 3,\n",
    "    'fun': 4,\n",
    "    'AI': 5\n",
    "}\n",
    "```\n",
    "\n",
    "Note: The actual integer values may be different depending on the special tokens you add to the vocabulary (like `<unk>`, `<pad>`, `<sos>`, and `<eos>`), but the concept is the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e8efc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build English vocabulary\n",
    "en_vocab = build_vocab_from_iterator(yield_tokens(df['english'], en_tokenizer), specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
    "fr_vocab = build_vocab_from_iterator(yield_tokens(df['french'], fr_tokenizer), specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
    "\n",
    "# Default index is the index of <unk>\n",
    "en_vocab.set_default_index(en_vocab['<unk>'])\n",
    "fr_vocab.set_default_index(fr_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3915e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 14372, first 10 tokens: ['<pad>', '<sos>', '<eos>', '<unk>', 'i', 'you', 'to', 'the', 'a', 'do']\n",
      "French vocabulary size: 24666, first 10 tokens: ['<pad>', '<sos>', '<eos>', '<unk>', 'je', 'de', 'pas', 'est', 'que', 'à']\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the vocabularies and some first tokens\n",
    "print(f'English vocabulary size: {len(en_vocab)}, first 10 tokens: {list(en_vocab.get_itos())[:10]}')\n",
    "print(f'French vocabulary size: {len(fr_vocab)}, first 10 tokens: {list(fr_vocab.get_itos())[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab652e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_transform(vocab: Vocab, tokenizer: Callable[[str], List[str]], text: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Transform a text into a list of integers.\n",
    "    \n",
    "    Args:\n",
    "        vocab (Vocab): the vocabulary\n",
    "        tokenizer (Callable[[str], List[str]]): the tokenizer\n",
    "        text (str): the input text\n",
    "        \n",
    "    Returns:\n",
    "        List[int]: the list of integers\n",
    "    \"\"\"\n",
    "    \n",
    "    return [vocab[token] for token in tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "845a2334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185583/185583 [00:02<00:00, 80292.56it/s]\n",
      "100%|██████████| 185583/185583 [00:03<00:00, 52662.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the text transforms by adding <sos> and <eos> tokens, and converting the text to a list of integers\n",
    "text_transform_en = lambda text: [en_vocab['<sos>']] + text_transform(en_vocab, en_tokenizer, text) + [en_vocab['<eos>']]\n",
    "text_transform_fr = lambda text: [fr_vocab['<sos>']] + text_transform(fr_vocab, fr_tokenizer, text) + [fr_vocab['<eos>']]\n",
    "\n",
    "# Transform the English and French sentences\n",
    "df['english_transform'] = df['english'].progress_apply(text_transform_en)\n",
    "df['french_transform'] = df['french'].progress_apply(text_transform_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e53c0733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "      <th>english_transform</th>\n",
       "      <th>french_transform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179927</th>\n",
       "      <td>i ca concentrate if you keep tapping me on the...</td>\n",
       "      <td>je ne peux pas me concentrer si tu continues d...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "      <td>[1, 4, 50, 1937, 63, 5, 178, 9793, 19, 34, 7, ...</td>\n",
       "      <td>[1, 4, 10, 57, 6, 27, 1811, 49, 15, 5663, 5, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50731</th>\n",
       "      <td>where are your things</td>\n",
       "      <td>où sont tes affaires</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "      <td>[1, 86, 23, 26, 208, 2]</td>\n",
       "      <td>[1, 75, 51, 185, 627, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113343</th>\n",
       "      <td>tom has too many strange ideas</td>\n",
       "      <td>tom a trop idées étranges</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "      <td>[1, 11, 62, 96, 129, 656, 971, 2]</td>\n",
       "      <td>[1, 16, 17, 109, 1007, 3896, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46173</th>\n",
       "      <td>how is your new class</td>\n",
       "      <td>comment est ta nouvelle classe</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "      <td>[1, 41, 10, 26, 144, 539, 2]</td>\n",
       "      <td>[1, 77, 7, 129, 302, 582, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150522</th>\n",
       "      <td>such a thing has never happened before</td>\n",
       "      <td>est du jamais vu</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n",
       "      <td>[1, 306, 8, 216, 62, 91, 172, 145, 2]</td>\n",
       "      <td>[1, 7, 42, 73, 139, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "179927  i ca concentrate if you keep tapping me on the...   \n",
       "50731                               where are your things   \n",
       "113343                     tom has too many strange ideas   \n",
       "46173                               how is your new class   \n",
       "150522             such a thing has never happened before   \n",
       "\n",
       "                                                   french  \\\n",
       "179927  je ne peux pas me concentrer si tu continues d...   \n",
       "50731                                où sont tes affaires   \n",
       "113343                          tom a trop idées étranges   \n",
       "46173                      comment est ta nouvelle classe   \n",
       "150522                                   est du jamais vu   \n",
       "\n",
       "                                              attribution  \\\n",
       "179927  CC-BY 2.0 (France) Attribution: tatoeba.org #1...   \n",
       "50731   CC-BY 2.0 (France) Attribution: tatoeba.org #1...   \n",
       "113343  CC-BY 2.0 (France) Attribution: tatoeba.org #5...   \n",
       "46173   CC-BY 2.0 (France) Attribution: tatoeba.org #3...   \n",
       "150522  CC-BY 2.0 (France) Attribution: tatoeba.org #8...   \n",
       "\n",
       "                                        english_transform  \\\n",
       "179927  [1, 4, 50, 1937, 63, 5, 178, 9793, 19, 34, 7, ...   \n",
       "50731                             [1, 86, 23, 26, 208, 2]   \n",
       "113343                  [1, 11, 62, 96, 129, 656, 971, 2]   \n",
       "46173                        [1, 41, 10, 26, 144, 539, 2]   \n",
       "150522              [1, 306, 8, 216, 62, 91, 172, 145, 2]   \n",
       "\n",
       "                                         french_transform  \n",
       "179927  [1, 4, 10, 57, 6, 27, 1811, 49, 15, 5663, 5, 2...  \n",
       "50731                            [1, 75, 51, 185, 627, 2]  \n",
       "113343                    [1, 16, 17, 109, 1007, 3896, 2]  \n",
       "46173                        [1, 77, 7, 129, 302, 582, 2]  \n",
       "150522                             [1, 7, 42, 73, 139, 2]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print random 5 rows\n",
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc86a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the maximum length of the English and French sentences\n",
    "en_max_len = df['english_transform'].apply(len).max()\n",
    "fr_max_len = df['french_transform'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bba60ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of English sentences: 46\n",
      "Maximum length of French sentences: 57\n"
     ]
    }
   ],
   "source": [
    "print(f'Maximum length of English sentences: {en_max_len}')\n",
    "print(f'Maximum length of French sentences: {fr_max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a84cb",
   "metadata": {},
   "source": [
    "The `pad_sequence` function is used to ensure that all sequences in a batch have the same length by padding shorter sequences with a specific value, usually 0.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. The function takes two arguments: `sequence`, which is a list of integers representing a sequence, and `max_length`, which is the desired length for all sequences.\n",
    "\n",
    "2. The function checks if the length of `sequence` is less than `max_length`.\n",
    "\n",
    "3. If it is, the function appends the padding value (`<pad>` token, represented by the integer 0) to `sequence` until its length is equal to `max_length`.\n",
    "\n",
    "4. The function then returns the padded sequence.\n",
    "\n",
    "Here's a visualization example:\n",
    "\n",
    "Suppose we have the following sequence and max_length:\n",
    "\n",
    "```python\n",
    "sequence = [1, 3, 2]\n",
    "max_length = 5\n",
    "```\n",
    "\n",
    "The `pad_sequence` function will pad the sequence with 0s until its length is 5:\n",
    "\n",
    "```python\n",
    "padded_sequence = [0, 0, 1, 3, 2]\n",
    "```\n",
    "\n",
    "This is useful in batch processing where all sequences need to have the same length for the computations to work. The padding value 0 is typically ignored by the model during training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b35b64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence: List[int], max_len: int, vocab: Vocab, pad_first: bool = True) -> List[int]:\n",
    "    \"\"\"\n",
    "    Pad a sequence with <pad> tokens.\n",
    "    \n",
    "    Args:\n",
    "        sequence (List[int]): the input sequence\n",
    "        max_len (int): the maximum length\n",
    "        vocab (Vocab): the vocabulary\n",
    "        pad_first (bool): whether to pad at the beginning or the end\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: the padded sequence as a tensor of long integers\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the number of tokens to pad\n",
    "    pad_len = max_len - len(sequence)\n",
    "    \n",
    "    # Pad the sequence\n",
    "    if pad_first:\n",
    "        sequence = [vocab['<pad>']] * pad_len + sequence\n",
    "    else:\n",
    "        sequence = sequence + [vocab['<pad>']] * pad_len\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfdeff",
   "metadata": {},
   "source": [
    "In LSTM models, the order of the sequence matters because the LSTM maintains an internal state that is updated for each element in the sequence. If you pad at the end of the sequence, the LSTM will update its state based on these padding tokens, which are meaningless and could potentially lead to less accurate predictions.\n",
    "\n",
    "On the other hand, if you pad at the beginning of the sequence, the LSTM will start updating its state based on the meaningful tokens right away, as soon as it encounters them. The padding tokens at the beginning of the sequence will have less impact on the final state of the LSTM, leading to more accurate predictions.\n",
    "\n",
    "This is especially important when using LSTM models with a fixed maximum sequence length, where sequences shorter than the maximum length need to be padded. By padding at the beginning of the sequence, you ensure that the LSTM's state is influenced as much as possible by the meaningful tokens in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e933c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185583/185583 [00:00<00:00, 765287.50it/s]\n",
      "100%|██████████| 185583/185583 [00:00<00:00, 423571.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pad the English and French sentences\n",
    "# We pad first for the English sentences, and pad last for the French sentences\n",
    "df['english_transform'] = df['english_transform'].progress_apply(lambda x: pad_sequence(x, en_max_len, en_vocab, True))\n",
    "df['french_transform'] = df['french_transform'].progress_apply(lambda x: pad_sequence(x, fr_max_len, fr_vocab, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a397dd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>attribution</th>\n",
       "      <th>english_transform</th>\n",
       "      <th>french_transform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179927</th>\n",
       "      <td>i ca concentrate if you keep tapping me on the...</td>\n",
       "      <td>je ne peux pas me concentrer si tu continues d...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 4, 10, 57, 6, 27, 1811, 49, 15, 5663, 5, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50731</th>\n",
       "      <td>where are your things</td>\n",
       "      <td>où sont tes affaires</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 75, 51, 185, 627, 2, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113343</th>\n",
       "      <td>tom has too many strange ideas</td>\n",
       "      <td>tom a trop idées étranges</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 16, 17, 109, 1007, 3896, 2, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46173</th>\n",
       "      <td>how is your new class</td>\n",
       "      <td>comment est ta nouvelle classe</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 77, 7, 129, 302, 582, 2, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150522</th>\n",
       "      <td>such a thing has never happened before</td>\n",
       "      <td>est du jamais vu</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 7, 42, 73, 139, 2, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "179927  i ca concentrate if you keep tapping me on the...   \n",
       "50731                               where are your things   \n",
       "113343                     tom has too many strange ideas   \n",
       "46173                               how is your new class   \n",
       "150522             such a thing has never happened before   \n",
       "\n",
       "                                                   french  \\\n",
       "179927  je ne peux pas me concentrer si tu continues d...   \n",
       "50731                                où sont tes affaires   \n",
       "113343                          tom a trop idées étranges   \n",
       "46173                      comment est ta nouvelle classe   \n",
       "150522                                   est du jamais vu   \n",
       "\n",
       "                                              attribution  \\\n",
       "179927  CC-BY 2.0 (France) Attribution: tatoeba.org #1...   \n",
       "50731   CC-BY 2.0 (France) Attribution: tatoeba.org #1...   \n",
       "113343  CC-BY 2.0 (France) Attribution: tatoeba.org #5...   \n",
       "46173   CC-BY 2.0 (France) Attribution: tatoeba.org #3...   \n",
       "150522  CC-BY 2.0 (France) Attribution: tatoeba.org #8...   \n",
       "\n",
       "                                        english_transform  \\\n",
       "179927  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50731   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "113343  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "46173   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "150522  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         french_transform  \n",
       "179927  [1, 4, 10, 57, 6, 27, 1811, 49, 15, 5663, 5, 2...  \n",
       "50731   [1, 75, 51, 185, 627, 2, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "113343  [1, 16, 17, 109, 1007, 3896, 2, 0, 0, 0, 0, 0,...  \n",
       "46173   [1, 77, 7, 129, 302, 582, 2, 0, 0, 0, 0, 0, 0,...  \n",
       "150522  [1, 7, 42, 73, 139, 2, 0, 0, 0, 0, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print random 5 rows\n",
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff12804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english, french):\n",
    "        self.english = english.apply(lambda x: torch.tensor(x, dtype=torch.long))\n",
    "        self.french = french.apply(lambda x: torch.tensor(x, dtype=torch.long))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.english)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'english': self.english[idx],\n",
    "            'french': self.french[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8038150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df: pd.DataFrame, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Create the datasets\n",
    "        self.translation_dataset = TranslationDataset(self.df['english_transform'], self.df['french_transform'])\n",
    "        \n",
    "        # Train/validation split\n",
    "        train_size = int(0.8 * len(self.df))\n",
    "        val_size = len(self.df) - train_size\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(self.translation_dataset, [train_size, val_size])\n",
    "        \n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46974c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data module\n",
    "data_module = TranslationDataModule(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af920db",
   "metadata": {},
   "source": [
    "### Seq2Seq Model\n",
    "\n",
    "The **Seq2Seq model**, also known as the **Sequence-to-Sequence model**, is a type of model that converts an input sequence into an output sequence. It's widely used in tasks such as machine translation, speech recognition, and more.\n",
    "\n",
    "The Seq2Seq model consists of two main components:\n",
    "\n",
    "1. **Encoder**: The encoder processes the input sequence and returns its own internal state. For each input element, the encoder updates its state. After processing the entire input sequence, the encoder outputs its final state, which serves as the \"context\" of the sequence.\n",
    "\n",
    "2. **Decoder**: The decoder uses the context (the final state of the encoder) to produce the output sequence. The decoder is also a recurrent network, and it produces the output sequence element by element. For each step, the input to the decoder is the previous element, the output of the decoder from the previous step, and the context.\n",
    "\n",
    "Here's a visualization of the Seq2Seq model:\n",
    "\n",
    "```\n",
    "Input Sequence -> | Encoder | -> Context -> | Decoder | -> Output Sequence\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6671f",
   "metadata": {},
   "source": [
    "#### Embedding Layer\n",
    "\n",
    "The embedding layer in a neural network is used to transform sparse categorical data, like words in a text dataset, into a dense vector representation that the network can work with. There are two main ways to use the embedding layer:\n",
    "\n",
    "1. **Self-Trained Embeddings**: In this case, the embedding layer is initialized with random weights and learns an embedding for each word in the vocabulary during the training of the network. This is a good option when you don't have a lot of domain-specific knowledge about the relationships between your categories.\n",
    "\n",
    "2. **Pre-Trained Embeddings**: In this case, the embedding layer is initialized with the weights from a pre-trained embedding, like Word2Vec or GloVe. These embeddings are trained on large corpora and can capture a lot of semantic information about words. This is a good option when your dataset is small and you want to leverage external knowledge.\n",
    "\n",
    "Here's example code for these two cases:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Vocabulary size and embedding dimension\n",
    "vocab_size = 5000\n",
    "embed_dim = 300\n",
    "\n",
    "# 1. Self-Trained Embeddings\n",
    "self_trained_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# 2. Pre-Trained Embeddings\n",
    "# Load pre-trained embeddings (replace with actual code to load your embeddings)\n",
    "pretrained_embeddings = torch.randn(vocab_size, embed_dim)  # Note: we need to load from a pretrained model\n",
    "pretrained_embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "```\n",
    "\n",
    "In the self-trained embeddings example, the `nn.Embedding` layer is initialized with random weights. In the pre-trained embeddings example, the `nn.Embedding` layer is initialized with weights from `pretrained_embeddings`, which is a tensor that you would typically load from a pre-trained embedding file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db415402",
   "metadata": {},
   "source": [
    "#### Freezing the Pre-Trained Embedding Layer\n",
    "\n",
    "When using pre-trained embeddings, we have two options:\n",
    "\n",
    "1. **Freeze the Embedding Layer**: In this case, the weights of the pre-trained embedding layer are kept constant during training. This means that the semantic information captured by the pre-trained embeddings is preserved, and the model cannot modify these embeddings to better fit the training data. This is a good option when your dataset is small and you want to leverage the semantic information in the pre-trained embeddings as much as possible.\n",
    "\n",
    "2. **Fine-Tune the Embedding Layer**: In this case, the weights of the pre-trained embedding layer are updated during training. This means that the model can modify these embeddings to better fit the training data. This is a good option when your dataset is large and you believe that the pre-trained embeddings may not be optimal for your specific task.\n",
    "\n",
    "You can decide whether to freeze the pre-trained embedding layer by setting the `requires_grad` attribute of the embedding layer's parameters. If `requires_grad` is `False`, the parameters are frozen and will not be updated during training. If `requires_grad` is `True`, the parameters will be updated during training.\n",
    "\n",
    "Here's example code showing how to freeze and unfreeze the pre-trained embedding layer:\n",
    "\n",
    "```python\n",
    "# Freeze the pre-trained embedding layer\n",
    "for param in pretrained_embedding.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the pre-trained embedding layer\n",
    "for param in pretrained_embedding.parameters():\n",
    "    param.requires_grad = True\n",
    "```\n",
    "\n",
    "In the first block of code, the `requires_grad` attribute of the embedding layer's parameters is set to `False`, freezing the parameters. In the second block of code, the `requires_grad` attribute is set to `True`, allowing the parameters to be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b037a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [26:28, 543kB/s]                                \n",
      "100%|█████████▉| 399999/400000 [00:19<00:00, 20402.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# For English, we use the existing GloVe embedding from torchtext\n",
    "en_glove = GloVe(name='6B', dim=300)\n",
    "\n",
    "# Create the embedding matrix\n",
    "en_embedding_matrix = en_glove.get_vecs_by_tokens(en_vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69abc97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if the word embedding file for French exists\n",
    "if not os.path.exists('Data/wiki.multi.fr.vec'):\n",
    "    # Download the word embedding file for French\n",
    "    !curl -o Data/wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1fccb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('Data/wiki.multi.fr.vec')\n",
    "\n",
    "# Get the number of words in the model's vocabulary and the size of the embeddings\n",
    "embed_size = word2vec_model.vector_size\n",
    "\n",
    "# Get the list of words in the vocabulary\n",
    "fr_vocab_words = fr_vocab.get_itos()\n",
    "\n",
    "# Initialize embedding matrix\n",
    "fr_embedding_matrix = torch.zeros(len(fr_vocab_words), embed_size)\n",
    "\n",
    "# Fill in the embedding matrix\n",
    "for i, word in enumerate(fr_vocab_words):\n",
    "    # Check if the word is in the Word2Vec model's vocabulary\n",
    "    if word in word2vec_model:\n",
    "        fr_embedding_matrix[i] = torch.tensor(word2vec_model[word])\n",
    "    else:\n",
    "        # If the word is not in the Word2Vec model's vocabulary, leave its embedding as zeros\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8867cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(self, en_embedding_matrix, fr_embedding_matrix, hidden_size, output_size, max_output_len):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \n",
    "        Args:\n",
    "            en_embedding_matrix (torch.Tensor): the embedding matrix for English\n",
    "            fr_embedding_matrix (torch.Tensor): the embedding matrix for French\n",
    "            hidden_size (int): the hidden size\n",
    "            output_size (int): the output size\n",
    "            max_output_len (int): the maximum output length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Maximum output length\n",
    "        self.max_output_len = max_output_len\n",
    "        \n",
    "        # Special tokens\n",
    "        # These tokens are the same in French and English dictionaries\n",
    "        self.sos_token = fr_vocab['<sos>']\n",
    "        self.eos_token = fr_vocab['<eos>']\n",
    "        self.pad_token = fr_vocab['<pad>']\n",
    "        \n",
    "        # Embedding layers\n",
    "        # We allow the models to update the embeddings, so we don't need to freeze them\n",
    "        self.en_embedding = nn.Embedding.from_pretrained(en_embedding_matrix, freeze=False)\n",
    "        self.fr_embedding = nn.Embedding.from_pretrained(fr_embedding_matrix, freeze=False)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=en_embedding_matrix.size(1),     # 300\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True    # This makes the data shape (batch size, sequence length, features)\n",
    "        )\n",
    "        self.encoder_dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=fr_embedding_matrix.size(1),     # 300\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True    # This makes the data shape (batch size, sequence length, features)\n",
    "        )\n",
    "        self.decoder_leaky_relu = nn.LeakyReLU()\n",
    "        self.decoder_fc = nn.Linear(hidden_size, output_size)   # 300 -> 25k of the French vocab size. This is to choose the index with the highest probability.\n",
    "        \n",
    "    def _encoder_forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): the input tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: the output tensor\n",
    "            torch.Tensor: the hidden state\n",
    "            torch.Tensor: the cell state\n",
    "        \"\"\"\n",
    "        # Embed the input\n",
    "        x = self.en_embedding(x)\n",
    "        \n",
    "        # Dropout\n",
    "        en_embedded = self.encoder_dropout(x)\n",
    "        \n",
    "        # Pass through the encoder LSTM\n",
    "        encoder_output, (hidden_state, cell_state) = self.encoder(en_embedded)\n",
    "        \n",
    "        # Return the output, hidden state, and cell state\n",
    "        return encoder_output, (hidden_state, cell_state)\n",
    "    \n",
    "    def _decoder_forward(self, x, hidden_and_cell):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): the input tensor\n",
    "            hidden_and_cell (tuple): the hidden state and cell state\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: the output tensor\n",
    "            torch.Tensor: the hidden state\n",
    "            torch.Tensor: the cell state\n",
    "        \"\"\"\n",
    "        # Embed the input\n",
    "        x = self.fr_embedding(x)\n",
    "        \n",
    "        # Unpack the hidden and state\n",
    "        hidden_state, cell_state = hidden_and_cell\n",
    "        \n",
    "        # Pass through the decoder LSTM\n",
    "        # The decoder_output is the hidden state of the last layer of the LSTM\n",
    "        # The shape is (batch size, sequence length, hidden size)\n",
    "        decoder_output, (hidden_state, cell_state) = self.decoder(x, (hidden_state, cell_state))\n",
    "        \n",
    "        # Because each time we run the _decoder_forward, we only pass one token, the sequence length is 1\n",
    "        # It means that the decoder_output shape is (batch size, 1, hidden size)\n",
    "        # We want it to become (batch size, hidden size) so that we can pass it to the fully connected layer\n",
    "        decoder_output = decoder_output.squeeze(1)\n",
    "        \n",
    "        # Pass through the fully connected layer to predict the next token\n",
    "        decoder_output = self.decoder_fc(decoder_output)        # Shape (batch size, output size)\n",
    "        \n",
    "        # Return the output, hidden state, and cell state\n",
    "        return decoder_output, (hidden_state, cell_state)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Seq2Seq model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): the input tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: the output tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass the input through the encoder\n",
    "        encoder_output, (hidden_state, cell_state) = self._encoder_forward(x)\n",
    "        \n",
    "        # Get the batch size\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Prepare the input for the decoder\n",
    "        # The shape of the input should be (batch size, 1) because we are passing one token at a time\n",
    "        decoder_input = torch.tensor([[self.sos_token]] * batch_size).to(x.device)\n",
    "        \n",
    "        # Create a list to store the outputs\n",
    "        decoder_outputs = []\n",
    "        for _ in range(self.max_output_len):\n",
    "            # Pass the input through the decoder\n",
    "            # The shape of the output is (batch size, output size)\n",
    "            decoder_output, (hidden_state, cell_state) = self._decoder_forward(decoder_input, (hidden_state, cell_state))\n",
    "            \n",
    "            # Add the output to the list\n",
    "            # The purpose of this storage is to calculate the loss later\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            \n",
    "            # Get the predicted token\n",
    "            # The predicted_token is the index of the token with the highest probability\n",
    "            predicted_token = decoder_output.argmax(1)\n",
    "            \n",
    "            # Detach the predicted token so that we can use it as input for the next iteration, and the model does not update its gradients\n",
    "            decoder_input = predicted_token.detach()\n",
    "            \n",
    "            # Reshape the predicted token to (batch size, 1) so that it can be passed to the decoder in the next iteration\n",
    "            decoder_input = decoder_input.unsqueeze(1)\n",
    "            \n",
    "        # Stack the decoder outputs\n",
    "        decoder_outputs = torch.stack(decoder_outputs, dim=1)\n",
    "        \n",
    "        return decoder_outputs\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get the input and target\n",
    "        x = batch['english']\n",
    "        y = batch['french']\n",
    "        \n",
    "        # Get the output\n",
    "        # The shape of the output is (batch_size, max_output_len, output_size)\n",
    "        output = self(x)\n",
    "        \n",
    "        # Reshape the output to (batch_size * max_output_len, output_size)\n",
    "        # This is because we want to calculate the loss for each word\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        \n",
    "        # Reshape the target to (batch_size * max_output_len)\n",
    "        # This is because we want to calculate the loss for each word (the target is the next word)\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=self.pad_token)(output, y)\n",
    "        \n",
    "        # Log the loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get the input and target\n",
    "        x = batch['english']\n",
    "        y = batch['french']\n",
    "        \n",
    "        # Get the output\n",
    "        # The shape of the output is (batch_size, max_output_len, output_size)\n",
    "        output = self(x)\n",
    "        \n",
    "        # Reshape the output to (batch_size * max_output_len, output_size)\n",
    "        # This is because we want to calculate the loss for each word\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        \n",
    "        # Reshape the target to (batch_size * max_output_len)\n",
    "        # This is because we want to calculate the loss for each word (the target is the next word)\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=self.pad_token)(output, y)\n",
    "        \n",
    "        # Log the loss\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c35049ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "seq2seq = Seq2Seq(en_embedding_matrix, fr_embedding_matrix, hidden_size=300, output_size=len(fr_vocab), max_output_len=fr_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8048917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stop_callback = pl.pytorch.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=True)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_callback = pl.pytorch.callbacks.ModelCheckpoint(monitor='val_loss', mode='min', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da2419a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "trainer = pl.Trainer(max_epochs=100, devices=-1, callbacks=[early_stop_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fa97caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# trainer.fit(seq2seq, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ea4f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "seq2seq = Seq2Seq.load_from_checkpoint(\n",
    "    \"./lightning_logs/version_26/checkpoints/epoch=9-step=11600.ckpt\",\n",
    "    en_embedding_matrix=en_embedding_matrix,\n",
    "    fr_embedding_matrix=fr_embedding_matrix,\n",
    "    hidden_size=300,\n",
    "    output_size=len(fr_vocab),\n",
    "    max_output_len=fr_max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c97a4b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "449b1900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a8cf0a17df61d9a0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a8cf0a17df61d9a0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af586b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Translate a sentence from English to French.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): the input sentence\n",
    "        \n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = ' '.join([token.text.lower() for token in nlp_en.tokenizer(sentence) if token.is_alpha])\n",
    "    \n",
    "    # Transform the sentence\n",
    "    transformed = text_transform_en(tokens)\n",
    "    \n",
    "    # Pad the sentence\n",
    "    padded = pad_sequence(transformed, en_max_len, en_vocab, pad_first=True)\n",
    "    \n",
    "    # Convert the sentence to a tensor\n",
    "    tensor = torch.tensor(padded, dtype=torch.long).unsqueeze(0).to(seq2seq.device)\n",
    "    \n",
    "    # Get the output\n",
    "    output = seq2seq(tensor)\n",
    "    \n",
    "    # Get the predicted words\n",
    "    predicted = output.argmax(dim=-1).squeeze(0)\n",
    "    \n",
    "    # Convert the predicted words to a list of integers\n",
    "    predicted = predicted.tolist()\n",
    "    \n",
    "    # Remove the <sos> token\n",
    "    predicted = predicted[1:]\n",
    "    \n",
    "    # Remove the <eos> token\n",
    "    predicted = predicted[:predicted.index(fr_vocab['<eos>'])]\n",
    "    \n",
    "    # Convert the integers to words\n",
    "    predicted = [fr_vocab.get_itos()[idx] for idx in predicted]\n",
    "    \n",
    "    # Join the words\n",
    "    predicted = ' '.join(predicted)\n",
    "    \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "465abd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je suis étudiant'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate some sentences\n",
    "translate_sentence('I am a student.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d048571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'il y a un chat sur la chat'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate some sentences\n",
    "translate_sentence('There is a cat on the table.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8ff3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a translation function that translate from token IDs to token IDs\n",
    "def translate_sentence_ids(sentence: List[int]):\n",
    "    \"\"\"\n",
    "    Translate a sentence from English to French.\n",
    "    \n",
    "    Args:\n",
    "        sentence (List[int]): the input sentence\n",
    "        \n",
    "    Returns:\n",
    "        List[int]: the translated sentence\n",
    "    \"\"\"    \n",
    "    # Convert the sentence to a tensor\n",
    "    tensor = torch.tensor(sentence, dtype=torch.long).to(seq2seq.device)\n",
    "    \n",
    "    # Get the output\n",
    "    output = seq2seq(tensor)\n",
    "    \n",
    "    # Get the predicted words\n",
    "    predicted = output.argmax(dim=-1).squeeze(0)\n",
    "    \n",
    "    # Convert the predicted words to a list of integers\n",
    "    predicted = predicted.tolist()\n",
    "    \n",
    "    # Remove the <sos> token\n",
    "    predicted = predicted[1:]\n",
    "    \n",
    "    # Remove the <eos> token\n",
    "    predicted = predicted[:predicted.index(fr_vocab['<eos>'])]\n",
    "        \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4a5abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/67jzbwsd73b2jw0q5xh7kft40000gn/T/ipykernel_62771/3239707536.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(sentence, dtype=torch.long).to(seq2seq.device)\n",
      "100%|██████████| 1160/1160 [21:56<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.3417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "# Create the BLEU score metric\n",
    "bleu_score = BLEUScore()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "seq2seq.eval()\n",
    "\n",
    "# Loop through the validation dataloader\n",
    "for batch in tqdm(data_module.val_dataloader()):\n",
    "    # Get the input and target\n",
    "    x = batch['english']\n",
    "    y = batch['french']\n",
    "    \n",
    "    # Translate the input sentences\n",
    "    translated_sentences = []\n",
    "    for sentence in x:\n",
    "        translated = translate_sentence_ids(sentence.unsqueeze(0))\n",
    "        translated_sentences.append(translated)\n",
    "    \n",
    "    # Process the target sentences\n",
    "    target_sentences = []\n",
    "    for sentence in y:\n",
    "        # Remove padding, <sos>, and everything after <eos>\n",
    "        cleaned = sentence[sentence != fr_vocab['<pad>']][1:]\n",
    "        if fr_vocab['<eos>'] in cleaned:\n",
    "            cleaned = cleaned[:cleaned.tolist().index(fr_vocab['<eos>'])]\n",
    "        target_sentences.append(cleaned.tolist())\n",
    "    \n",
    "    # Convert token IDs to words and join them into strings\n",
    "    translated_sentences = [' '.join([fr_vocab.get_itos()[idx] for idx in sent]) for sent in translated_sentences]\n",
    "    target_sentences = [[' '.join([fr_vocab.get_itos()[idx] for idx in sent])] for sent in target_sentences]\n",
    "    \n",
    "    # Update the BLEU score\n",
    "    bleu_score.update(translated_sentences, target_sentences)\n",
    "\n",
    "# Compute the final BLEU score\n",
    "final_bleu_score = bleu_score.compute()\n",
    "print(f\"BLEU Score: {final_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c6c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttth_natural_language_processing_practice_k302",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
